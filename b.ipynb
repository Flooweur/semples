{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. CLIP-based Semantic Segmentation\n",
    "Recently, the Contrastive Language-Image Pretraining\n",
    "(CLIP) model [51] has been adopted in semantic segmen-\n",
    "tation tasks thanks to the generalized knowledge learned\n",
    "from a large corpus of image-text pairs. Given the gener-\n",
    "alization capability, a number of zero-shot/open-vocabulary\n",
    "approaches [17, 22, 34, 38, 47, 52, 67, 70, 71] exploit CLIP to\n",
    "segment the classes which are unseen during training. How-\n",
    "ever, these methods still require mask annotations during\n",
    "training. To minimize the annotation effort, CLIP has also\n",
    "been adopted to improve unsupervised methods [24, 58, 81].\n",
    "Nevertheless, the segmentation performance is still unsat-\n",
    "isfactory and is not desired for further applications. On\n",
    "the other hand, CLIP has also been utilized to benefit\n",
    "WSSS [42, 49, 65, 69, 72]. These works mainly focus on\n",
    "designing text prompts or prompt learning techniques for\n",
    "the text encoder. However, they either consider only the\n",
    "foreground class prompts, or rely on general background\n",
    "prompts defined by additional manual efforts and heuris-\n",
    "tic human knowledge. Moreover, such manually-defined\n",
    "prompts may not fully exploit the knowledge in the CLIP\n",
    "latent space. In contrast, with no need for any manual\n",
    "efforts, our proposed SemPLeS framework automatically\n",
    "learns prompts embedded with class-associated semantic\n",
    "knowledge discovered from the CLIP latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 64, 64])\n",
      "torch.Size([2, 3, 64, 64])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "batch_size = 2\n",
    "channels = 3\n",
    "height = 64\n",
    "width = 64\n",
    "prompt_dim = 512\n",
    "\n",
    "# Create dummy inputs\n",
    "X_f = torch.randn(batch_size, channels, height, width)  # Foreground image\n",
    "X_b = torch.randn(batch_size, channels, height, width)  # Background image\n",
    "t_k = torch.randn(batch_size, prompt_dim)  # Text prompt embeddings\n",
    "p_k = torch.randn(batch_size, prompt_dim)  # Learnable background prompts\n",
    "\n",
    "print(X_f.shape)\n",
    "print(X_b.shape)\n",
    "print(t_k.shape)\n",
    "print(p_k.shape)\n",
    "\n",
    "# Define ContrastivePromptLearning class\n",
    "class ContrastivePromptLearning(nn.Module):\n",
    "    def __init__(self, image_encoder, text_encoder, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.E_I = image_encoder  # Image encoder\n",
    "        self.E_T = text_encoder  # Text encoder\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, X_f, X_b, t_k, p_k):\n",
    "        \"\"\"\n",
    "        Contrastive Prompt Learning to learn background prompts\n",
    "        Args:\n",
    "            X_f: Foreground image (B, C, H, W)\n",
    "            X_b: Background image (B, C, H, W) \n",
    "            t_k: Text prompt embeddings (B, prompt_dim)\n",
    "            p_k: Learnable background prompts (B, prompt_dim)\n",
    "        \"\"\"\n",
    "        # Flatten images before passing to linear encoder\n",
    "        batch_size = X_f.shape[0]\n",
    "        X_f_flat = X_f.view(batch_size, -1)  # Flatten to (B, C*H*W)\n",
    "        X_b_flat = X_b.view(batch_size, -1)  # Flatten to (B, C*H*W)\n",
    "        \n",
    "        # Get image embeddings using CLIP image encoder\n",
    "        v_f = self.E_I(X_f_flat)  # Foreground image embedding\n",
    "        v_b = self.E_I(X_b_flat)  # Background image embedding\n",
    "        \n",
    "        # Get text embeddings using CLIP text encoder\n",
    "        u_t = self.E_T(t_k)  # Text prompt embedding\n",
    "        u_b = self.E_T(p_k)  # Background prompt embedding\n",
    "\n",
    "        # Compute cosine similarities scaled by temperature\n",
    "        sim_f_t = F.cosine_similarity(v_f, u_t, dim=-1) / self.temperature\n",
    "        sim_b_p = F.cosine_similarity(v_b, u_b, dim=-1) / self.temperature\n",
    "        sim_f_p = F.cosine_similarity(v_f, u_b, dim=-1) / self.temperature\n",
    "        sim_b_t = F.cosine_similarity(v_b, u_t, dim=-1) / self.temperature\n",
    "\n",
    "        # Compute contrastive loss\n",
    "        L_contrast = -torch.log(torch.exp(sim_f_t) / (torch.exp(sim_f_t) + torch.exp(sim_f_p))) \\\n",
    "                    -torch.log(torch.exp(sim_b_p) / (torch.exp(sim_b_p) + torch.exp(sim_b_t)))\n",
    "        \n",
    "        return L_contrast.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContrastivePromptLearning test passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize model \n",
    "model = ContrastivePromptLearning(image_encoder, text_encoder)\n",
    "\n",
    "# Forward pass\n",
    "loss = model(X_f, X_b, t_k, p_k)\n",
    "\n",
    "# Check output\n",
    "assert isinstance(loss, torch.Tensor)\n",
    "assert loss.dim() == 0  # scalar\n",
    "assert not torch.isnan(loss)\n",
    "assert not torch.isinf(loss)\n",
    "\n",
    "print(\"ContrastivePromptLearning test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PngImageFile' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m image_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackground.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(image_path)\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Define image transforms\u001b[39;00m\n\u001b[0;32m     16\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     17\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)),\n\u001b[0;32m     18\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     19\u001b[0m ])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PngImageFile' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "import requests\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "\n",
    "# Create dummy encoders\n",
    "image_encoder = nn.Linear(64 * 64 * 3, prompt_dim)  # Changed from 3 * 64 * 64 to match flattened input size\n",
    "text_encoder = nn.Linear(prompt_dim, prompt_dim)\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = Path(\"background.png\")\n",
    "image = PIL.Image.open(image_path)\n",
    "print(image.shape)\n",
    "\n",
    "# Define image transforms\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Preprocess image\n",
    "img_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Create dummy background image by slightly perturbing original\n",
    "background = img_tensor + 0.1 * torch.randn_like(img_tensor)\n",
    "\n",
    "# Text prompt\n",
    "text_prompt = \"Photo of a train\"\n",
    "text_embedding = torch.randn(1, prompt_dim)  # Simulate text embedding\n",
    "background_prompt = torch.randn(1, prompt_dim)  # Learnable background prompt\n",
    "\n",
    "# Forward pass with real data\n",
    "loss = model(img_tensor, background, text_embedding, background_prompt)\n",
    "\n",
    "print(f\"Loss on real data: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
